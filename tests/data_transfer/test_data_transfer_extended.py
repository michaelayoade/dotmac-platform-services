"""
Extended tests for data transfer module to improve coverage.
Focuses on exporters, importers, and advanced functionality.
"""

import asyncio
import csv
import io
import json
import os
import tempfile
import zipfile
import gzip
from datetime import datetime, UTC
from pathlib import Path
from typing import Any, Dict, List
from unittest.mock import AsyncMock, Mock, MagicMock, patch, mock_open

import pytest

from dotmac.platform.data_transfer.base import (
    DataFormat,
    TransferStatus,
    CompressionType,
    ProgressInfo,
    DataRecord,
    DataBatch,
    TransferConfig,
    BaseExporter,
    BaseImporter,
)
from dotmac.platform.data_transfer.exporters import (
    ExportOptions,
    CSVExporter,
    JSONExporter,
    ExcelExporter,
    XMLExporter,
    YAMLExporter,
    CompressionMixin,
)
from dotmac.platform.data_transfer.importers import (
    ImportOptions,
    CSVImporter,
    JSONImporter,
    ExcelImporter,
    XMLImporter,
)
from dotmac.platform.data_transfer.progress import (
    ProgressStore,
    FileProgressStore,
    CheckpointData,
    CheckpointStore,
    ProgressTracker,
)


class TestCSVExporterExtended:
    """Extended CSV exporter tests."""

    @pytest.fixture
    def csv_exporter(self):
        """Create CSV exporter."""
        return CSVExporter()

    async def test_csv_with_nested_data(self, csv_exporter):
        """Test CSV export with nested data structures."""
        records = [
            DataRecord(
                id="1",
                data={
                    "name": "John",
                    "address": {"street": "123 Main", "city": "NYC"},
                    "scores": [95, 87, 92]
                }
            )
        ]

        output = io.StringIO()
        result = await csv_exporter.export(records, output)

        assert result.status == TransferStatus.COMPLETED
        output.seek(0)
        content = output.read()
        # Nested data should be flattened or serialized
        assert "John" in content

    async def test_csv_with_unicode_data(self, csv_exporter):
        """Test CSV export with unicode characters."""
        records = [
            DataRecord(
                id="1",
                data={
                    "name": "JosÃ© MarÃ­a",
                    "city": "SÃ£o Paulo",
                    "emoji": "ðŸš€ðŸ“Š"
                }
            )
        ]

        output = io.StringIO()
        result = await csv_exporter.export(records, output)

        assert result.status == TransferStatus.COMPLETED
        output.seek(0)
        content = output.read()
        assert "JosÃ© MarÃ­a" in content
        assert "SÃ£o Paulo" in content

    async def test_csv_custom_quoting(self):
        """Test CSV with custom quoting options."""
        options = ExportOptions(
            quoting=csv.QUOTE_ALL,
            quotechar="'",
            escapechar="\\"
        )
        exporter = CSVExporter(options=options)

        records = [
            DataRecord(id="1", data={"field": "value with, comma"})
        ]

        output = io.StringIO()
        await exporter.export(records, output)

        output.seek(0)
        content = output.read()
        assert "'" in content  # Should use single quotes

    async def test_csv_streaming_large_dataset(self, csv_exporter):
        """Test CSV streaming with large dataset."""
        async def record_generator():
            for i in range(10000):
                yield DataRecord(
                    id=str(i),
                    data={"index": i, "value": f"data_{i}"}
                )

        output = io.StringIO()

        if hasattr(csv_exporter, 'export_stream'):
            result = await csv_exporter.export_stream(record_generator(), output)
            assert result.status == TransferStatus.COMPLETED
            assert result.processed_records == 10000

    async def test_csv_with_null_values(self, csv_exporter):
        """Test CSV handling of null values."""
        records = [
            DataRecord(
                id="1",
                data={"field1": "value", "field2": None, "field3": ""}
            )
        ]

        output = io.StringIO()
        result = await csv_exporter.export(records, output)

        assert result.status == TransferStatus.COMPLETED


class TestJSONExporterExtended:
    """Extended JSON exporter tests."""

    @pytest.fixture
    def json_exporter(self):
        """Create JSON exporter."""
        return JSONExporter()

    async def test_json_with_datetime_objects(self, json_exporter):
        """Test JSON export with datetime objects."""
        records = [
            DataRecord(
                id="1",
                data={
                    "created_at": datetime.now(UTC),
                    "updated_at": datetime.now(UTC).isoformat()
                }
            )
        ]

        output = io.StringIO()
        result = await json_exporter.export(records, output)

        assert result.status == TransferStatus.COMPLETED
        output.seek(0)
        data = json.loads(output.read())
        # Datetime should be serialized
        assert "created_at" in data[0]

    async def test_json_compact_format(self):
        """Test JSON export in compact format."""
        options = ExportOptions(json_indent=None)
        exporter = JSONExporter(options=options)

        records = [
            DataRecord(id="1", data={"nested": {"deep": {"value": 123}}})
        ]

        output = io.StringIO()
        await exporter.export(records, output)

        output.seek(0)
        content = output.read()
        # Should be compact (no extra whitespace)
        assert "\n" not in content or content.count("\n") <= 1

    async def test_json_lines_format(self):
        """Test JSON Lines format export."""
        options = ExportOptions(json_lines=True)
        exporter = JSONExporter(options=options)

        records = [
            DataRecord(id=str(i), data={"value": i})
            for i in range(5)
        ]

        output = io.StringIO()
        await exporter.export(records, output)

        output.seek(0)
        lines = output.readlines()
        assert len(lines) == 5

        # Each line should be valid JSON
        for i, line in enumerate(lines):
            data = json.loads(line.strip())
            assert data["value"] == i

    async def test_json_with_custom_serializer(self):
        """Test JSON with custom serialization."""
        class CustomEncoder(json.JSONEncoder):
            def default(self, obj):
                if isinstance(obj, set):
                    return list(obj)
                return super().default(obj)

        records = [
            DataRecord(
                id="1",
                data={"tags": {"python", "testing", "json"}}  # Set object
            )
        ]

        # This would require the exporter to accept custom encoders
        # For now, test that it handles the data somehow
        exporter = JSONExporter()
        output = io.StringIO()

        try:
            result = await exporter.export(records, output)
            # May fail or serialize differently
            assert result is not None
        except (TypeError, ValueError):
            # Expected for non-serializable objects
            pass


class TestExcelExporterExtended:
    """Extended Excel exporter tests."""

    @pytest.fixture
    def excel_exporter(self):
        """Create Excel exporter."""
        return ExcelExporter()

    @patch('openpyxl.Workbook')
    async def test_excel_multiple_sheets(self, mock_workbook, excel_exporter):
        """Test Excel export with multiple sheets."""
        mock_wb = MagicMock()
        mock_ws1 = MagicMock()
        mock_ws2 = MagicMock()
        mock_workbook.return_value = mock_wb
        mock_wb.create_sheet.side_effect = [mock_ws1, mock_ws2]

        # Test if exporter supports multiple sheets
        records_sheet1 = [DataRecord(id="1", data={"sheet": "1"})]
        records_sheet2 = [DataRecord(id="2", data={"sheet": "2"})]

        with tempfile.NamedTemporaryFile(suffix='.xlsx') as tmp:
            if hasattr(excel_exporter, 'export_multiple_sheets'):
                await excel_exporter.export_multiple_sheets({
                    "Sheet1": records_sheet1,
                    "Sheet2": records_sheet2
                }, tmp.name)

    @patch('openpyxl.Workbook')
    async def test_excel_with_formulas(self, mock_workbook, excel_exporter):
        """Test Excel export with formulas."""
        mock_wb = MagicMock()
        mock_ws = MagicMock()
        mock_workbook.return_value = mock_wb
        mock_wb.active = mock_ws

        records = [
            DataRecord(
                id="1",
                data={"value1": 10, "value2": 20, "formula": "=A1+B1"}
            )
        ]

        with tempfile.NamedTemporaryFile(suffix='.xlsx') as tmp:
            result = await excel_exporter.export(records, tmp.name)
            assert result.status == TransferStatus.COMPLETED

    @patch('openpyxl.Workbook')
    async def test_excel_with_formatting(self, mock_workbook):
        """Test Excel export with cell formatting."""
        options = ExportOptions(
            excel_formatting={
                "number_format": "0.00",
                "font_bold": True,
                "background_color": "FFFF00"
            }
        )

        exporter = ExcelExporter(options=options)

        mock_wb = MagicMock()
        mock_ws = MagicMock()
        mock_workbook.return_value = mock_wb
        mock_wb.active = mock_ws

        records = [DataRecord(id="1", data={"amount": 123.456})]

        with tempfile.NamedTemporaryFile(suffix='.xlsx') as tmp:
            result = await exporter.export(records, tmp.name)
            assert result.status == TransferStatus.COMPLETED


class TestXMLExporterExtended:
    """Extended XML exporter tests."""

    @pytest.fixture
    def xml_exporter(self):
        """Create XML exporter."""
        return XMLExporter()

    async def test_xml_with_attributes(self, xml_exporter):
        """Test XML export with element attributes."""
        options = ExportOptions(
            xml_use_attributes=True,
            xml_root_element="catalog",
            xml_record_element="item"
        )
        exporter = XMLExporter(options=options)

        records = [
            DataRecord(
                id="item_001",
                data={"name": "Product 1", "price": 99.99},
                metadata={"category": "electronics", "brand": "TechCorp"}
            )
        ]

        output = io.StringIO()
        result = await exporter.export(records, output)

        assert result.status == TransferStatus.COMPLETED
        output.seek(0)
        content = output.read()

        assert "<catalog>" in content
        assert 'id="item_001"' in content or "item_001" in content

    async def test_xml_with_cdata(self, xml_exporter):
        """Test XML export with CDATA sections."""
        records = [
            DataRecord(
                id="1",
                data={
                    "description": "<p>This contains <b>HTML</b> & special chars</p>",
                    "code": "if (x < y && z > 0) { return true; }"
                }
            )
        ]

        output = io.StringIO()
        result = await xml_exporter.export(records, output)

        assert result.status == TransferStatus.COMPLETED
        output.seek(0)
        content = output.read()

        # Should handle special characters
        assert "&lt;" in content or "CDATA" in content or "<p>" in content

    async def test_xml_namespaces(self):
        """Test XML export with namespaces."""
        options = ExportOptions(
            xml_namespace="http://example.com/schema",
            xml_namespace_prefix="ex"
        )
        exporter = XMLExporter(options=options)

        records = [DataRecord(id="1", data={"field": "value"})]

        output = io.StringIO()
        result = await exporter.export(records, output)

        assert result.status == TransferStatus.COMPLETED
        output.seek(0)
        content = output.read()

        assert "xmlns" in content


class TestYAMLExporterExtended:
    """Extended YAML exporter tests."""

    @pytest.fixture
    def yaml_exporter(self):
        """Create YAML exporter."""
        return YAMLExporter()

    async def test_yaml_export_basic(self, yaml_exporter):
        """Test basic YAML export."""
        records = [
            DataRecord(
                id="1",
                data={
                    "name": "Test",
                    "config": {
                        "enabled": True,
                        "timeout": 30,
                        "hosts": ["host1", "host2"]
                    }
                }
            )
        ]

        output = io.StringIO()
        result = await yaml_exporter.export(records, output)

        assert result.status == TransferStatus.COMPLETED
        output.seek(0)
        content = output.read()

        assert "name: Test" in content
        assert "enabled: true" in content or "enabled: True" in content

    async def test_yaml_with_complex_structures(self, yaml_exporter):
        """Test YAML export with complex data structures."""
        records = [
            DataRecord(
                id="1",
                data={
                    "matrix": [[1, 2, 3], [4, 5, 6]],
                    "mappings": {
                        "env": {"dev": "localhost", "prod": "example.com"},
                        "ports": {"http": 80, "https": 443}
                    }
                }
            )
        ]

        output = io.StringIO()
        result = await yaml_exporter.export(records, output)

        assert result.status == TransferStatus.COMPLETED


class TestCompressionMixin:
    """Test compression functionality."""

    def test_gzip_compression(self):
        """Test GZIP compression."""
        mixin = CompressionMixin()
        data = b"This is test data that should compress well. " * 100

        if hasattr(mixin, 'compress_gzip'):
            compressed = mixin.compress_gzip(data)
            assert len(compressed) < len(data)

            # Test decompression
            decompressed = gzip.decompress(compressed)
            assert decompressed == data

    def test_zip_compression(self):
        """Test ZIP compression."""
        mixin = CompressionMixin()
        files = {
            "file1.txt": b"Content of file 1",
            "file2.txt": b"Content of file 2"
        }

        if hasattr(mixin, 'compress_zip'):
            compressed = mixin.compress_zip(files)

            # Should be valid ZIP
            with zipfile.ZipFile(io.BytesIO(compressed), 'r') as zf:
                assert "file1.txt" in zf.namelist()
                assert "file2.txt" in zf.namelist()

    def test_compression_ratio_calculation(self):
        """Test compression ratio calculation."""
        mixin = CompressionMixin()

        original_data = b"A" * 1000
        compressed_data = gzip.compress(original_data)

        if hasattr(mixin, 'calculate_compression_ratio'):
            ratio = mixin.calculate_compression_ratio(
                len(original_data),
                len(compressed_data)
            )
            assert ratio > 1.0  # Should be compressed


class TestImportersExtended:
    """Extended importer tests."""

    async def test_csv_importer_with_different_encodings(self):
        """Test CSV import with different encodings."""
        # Test UTF-8
        csv_data_utf8 = "name,city\nJosÃ©,SÃ£o Paulo\n"

        importer = CSVImporter()
        input_stream = io.StringIO(csv_data_utf8)

        records = []
        async for record in importer.import_stream(input_stream):
            records.append(record)

        assert len(records) == 1
        assert records[0].data["name"] == "JosÃ©"

    async def test_json_importer_with_streaming(self):
        """Test JSON importer with streaming large files."""
        importer = JSONImporter()

        # Create large JSON array
        large_data = [{"id": str(i), "value": i} for i in range(1000)]
        json_data = json.dumps(large_data)

        input_stream = io.StringIO(json_data)

        records = []
        async for record in importer.import_stream(input_stream):
            records.append(record)
            if len(records) >= 100:  # Limit for test
                break

        assert len(records) == 100

    async def test_xml_importer_with_nested_elements(self):
        """Test XML importer with nested elements."""
        xml_data = """<?xml version="1.0"?>
        <root>
            <record id="1">
                <person>
                    <name>John Doe</name>
                    <contact>
                        <email>john@example.com</email>
                        <phone>555-1234</phone>
                    </contact>
                </person>
            </record>
        </root>"""

        importer = XMLImporter()
        input_stream = io.StringIO(xml_data)

        records = []
        async for record in importer.import_stream(input_stream):
            records.append(record)

        assert len(records) == 1
        # Should handle nested structure
        assert "person" in records[0].data or "name" in records[0].data

    @patch('openpyxl.load_workbook')
    async def test_excel_importer_multiple_sheets(self, mock_load_workbook):
        """Test Excel importer with multiple sheets."""
        mock_wb = MagicMock()
        mock_ws1 = MagicMock()
        mock_ws2 = MagicMock()

        mock_ws1.iter_rows.return_value = [
            [MagicMock(value="col1"), MagicMock(value="col2")],
            [MagicMock(value="val1"), MagicMock(value="val2")]
        ]
        mock_ws2.iter_rows.return_value = [
            [MagicMock(value="col3"), MagicMock(value="col4")],
            [MagicMock(value="val3"), MagicMock(value="val4")]
        ]

        mock_wb.worksheets = [mock_ws1, mock_ws2]
        mock_wb.sheetnames = ["Sheet1", "Sheet2"]
        mock_load_workbook.return_value = mock_wb

        importer = ExcelImporter()

        with tempfile.NamedTemporaryFile(suffix='.xlsx') as tmp:
            tmp.write(b"fake excel data")
            tmp.flush()

            if hasattr(importer, 'import_all_sheets'):
                sheets_data = await importer.import_all_sheets(tmp.name)
                assert len(sheets_data) == 2


class TestProgressTrackingExtended:
    """Extended progress tracking tests."""

    async def test_progress_tracker_with_checkpoints(self):
        """Test progress tracking with checkpoints."""
        tracker = ProgressTracker()

        # Start operation
        tracker.start_operation("checkpoint_test")

        # Update with checkpoints
        for i in range(100):
            tracker.update_progress(
                "checkpoint_test",
                processed=i,
                total=100
            )

            if i % 10 == 0:  # Every 10 records
                if hasattr(tracker, 'create_checkpoint'):
                    tracker.create_checkpoint(
                        "checkpoint_test",
                        position=i,
                        state={"last_id": f"record_{i}"}
                    )

        # Should have multiple checkpoints
        if hasattr(tracker, 'get_checkpoints'):
            checkpoints = tracker.get_checkpoints("checkpoint_test")
            assert len(checkpoints) >= 5

    def test_progress_store_cleanup(self):
        """Test progress store cleanup of old data."""
        with tempfile.TemporaryDirectory() as tmpdir:
            store = FileProgressStore(storage_path=tmpdir)

            # Create multiple progress files
            for i in range(10):
                info = ProgressInfo(
                    operation_id=f"op_{i}",
                    processed_records=i * 10,
                    total_records=100,
                    status=TransferStatus.COMPLETED
                )
                store.save_progress(f"op_{i}", info)

            # Cleanup old completed operations
            if hasattr(store, 'cleanup_completed'):
                cleaned = store.cleanup_completed(max_age_days=0)
                assert cleaned > 0

    async def test_resumable_operation(self):
        """Test resumable operation functionality."""
        class TestResumableOperation:
            def __init__(self):
                self.processed = 0
                self.checkpoints = []

            async def process_batch(self, start, end):
                self.processed = end
                return f"Processed {start}-{end}"

            def create_checkpoint(self):
                return CheckpointData(
                    operation_id="test_op",
                    position=self.processed,
                    state={"processed": self.processed}
                )

            async def resume_from_checkpoint(self, checkpoint):
                self.processed = checkpoint.position

        operation = TestResumableOperation()

        # Process partially
        await operation.process_batch(0, 50)
        checkpoint = operation.create_checkpoint()

        # Simulate restart
        new_operation = TestResumableOperation()
        await new_operation.resume_from_checkpoint(checkpoint)

        assert new_operation.processed == 50

    def test_concurrent_progress_updates(self):
        """Test concurrent progress updates."""
        tracker = ProgressTracker()

        def update_progress(operation_id, start, end):
            for i in range(start, end):
                tracker.update_progress(
                    operation_id,
                    processed=i,
                    total=1000
                )

        import threading

        # Start multiple threads updating progress
        threads = []
        for i in range(0, 100, 20):
            thread = threading.Thread(
                target=update_progress,
                args=(f"concurrent_op_{i//20}", i, i+20)
            )
            threads.append(thread)
            thread.start()

        for thread in threads:
            thread.join()

        # Should have 5 different operations
        if hasattr(tracker, 'get_all_operations'):
            operations = tracker.get_all_operations()
            assert len(operations) == 5


class TestDataValidation:
    """Test data validation functionality."""

    def test_record_validation_success(self):
        """Test successful record validation."""
        record = DataRecord(
            id="valid_record",
            data={"email": "user@example.com", "age": 25}
        )

        # Basic validation should pass
        assert record.id == "valid_record"
        assert isinstance(record.data, dict)

    def test_record_validation_failure(self):
        """Test record validation failure."""
        # Test invalid record creation
        with pytest.raises(ValueError):
            DataRecord(id="", data={})  # Empty ID

    def test_batch_validation(self):
        """Test batch validation."""
        valid_records = [
            DataRecord(id=str(i), data={"value": i})
            for i in range(5)
        ]

        batch = DataBatch(records=valid_records)

        # Batch should validate all records
        assert batch.size == 5

    def test_custom_validator(self):
        """Test custom data validator."""
        def email_validator(record: DataRecord) -> bool:
            email = record.data.get("email", "")
            return "@" in email and "." in email

        record_valid = DataRecord(
            id="1",
            data={"email": "user@example.com"}
        )
        record_invalid = DataRecord(
            id="2",
            data={"email": "invalid-email"}
        )

        assert email_validator(record_valid) is True
        assert email_validator(record_invalid) is False


class TestErrorRecovery:
    """Test error recovery mechanisms."""

    async def test_partial_export_recovery(self):
        """Test recovery from partial export."""
        exporter = CSVExporter()

        records = [
            DataRecord(id=str(i), data={"value": i})
            for i in range(100)
        ]

        # Mock output that fails halfway
        class FailingOutput:
            def __init__(self):
                self.content = ""
                self.writes = 0

            def write(self, data):
                self.writes += 1
                if self.writes > 50:
                    raise IOError("Disk full")
                self.content += data
                return len(data)

        output = FailingOutput()

        try:
            await exporter.export(records, output)
        except Exception:
            pass

        # Should have partial content
        assert len(output.content) > 0
        assert output.writes == 51  # Failed on 51st write

    async def test_import_error_handling(self):
        """Test import error handling."""
        importer = JSONImporter()

        # Invalid JSON data
        invalid_json = '{"incomplete": json data'
        input_stream = io.StringIO(invalid_json)

        records = []
        try:
            async for record in importer.import_stream(input_stream):
                records.append(record)
        except json.JSONDecodeError:
            pass  # Expected

        # Should handle gracefully
        assert len(records) == 0  # No records processed due to error

    def test_checkpoint_corruption_recovery(self):
        """Test recovery from corrupted checkpoints."""
        with tempfile.TemporaryDirectory() as tmpdir:
            store = CheckpointStore(storage_path=tmpdir)

            # Create valid checkpoint
            checkpoint = CheckpointData(
                operation_id="test_op",
                position=100
            )
            store.save_checkpoint(checkpoint)

            # Corrupt the checkpoint file
            checkpoint_file = Path(tmpdir) / "test_op.checkpoint"
            with open(checkpoint_file, 'w') as f:
                f.write("corrupted data")

            # Should handle corruption gracefully
            try:
                loaded = store.load_checkpoint("test_op")
                assert loaded is None  # or handle differently
            except Exception:
                pass  # Expected for corrupted data